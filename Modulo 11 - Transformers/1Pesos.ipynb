{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cc20848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "347d43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função softmax simples\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93e6d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função que calcula a atenção simplificada\n",
    "def self_attention(Q, K, V):\n",
    "    # Similaridade (produto escalar entre Query e Key)\n",
    "    scores = np.dot(Q, K.T)\n",
    "    \n",
    "    # Aplicar softmax para obter pesos normalizados (probabilidades)\n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Multiplicar pesos pelos valores (Value)\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nossa frase:\n",
    "sentence = [\"Maria\", \"gosta\", \"de\", \"matemática\"]\n",
    "\n",
    "# Para fins didáticos, criamos embeddings artificiais (dimensionais simplificadas = 3)\n",
    "word_embeddings = {\n",
    "    \"Maria\":      [1.0, 0.2, 0.4],\n",
    "    \"gosta\":      [0.9, 0.1, 0.3],\n",
    "    \"de\":         [0.1, 1.0, 0.7],\n",
    "    \"matemática\": [0.8, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Query representa \"o que cada palavra busca\".\n",
    "# Key representa \"o que cada palavra oferece às outras\".\n",
    "# Value é \"a informação real de cada palavra\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1ac615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras: ['Maria', 'gosta', 'de', 'matemática']\n",
      "\n",
      "Pesos de Atenção:\n",
      " [[0.29 0.25 0.16 0.31]\n",
      " [0.29 0.26 0.15 0.29]\n",
      " [0.16 0.13 0.4  0.3 ]\n",
      " [0.23 0.19 0.22 0.36]]\n",
      "\n",
      "Representação final enriquecida por atenção:\n",
      " [[0.77 0.39 0.58]\n",
      " [0.78 0.39 0.57]\n",
      " [0.56 0.6  0.66]\n",
      " [0.71 0.47 0.63]]\n"
     ]
    }
   ],
   "source": [
    "# Montamos as matrizes Q, K e V a partir dos embeddings\n",
    "Q = np.array([word_embeddings[word] for word in sentence])\n",
    "K = np.array([word_embeddings[word] for word in sentence])\n",
    "V = np.array([word_embeddings[word] for word in sentence])\n",
    "\n",
    "# Executamos o mecanismo de atenção\n",
    "output, attention_weights = self_attention(Q, K, V)\n",
    "\n",
    "# Resultados:\n",
    "print(\"Palavras:\", sentence)\n",
    "print(\"\\nPesos de Atenção:\\n\", np.round(attention_weights, 2))\n",
    "print(\"\\nRepresentação final enriquecida por atenção:\\n\", np.round(output, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc48711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
